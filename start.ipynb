{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### importing libaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.display import display"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### reading data set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 55)\n"
     ]
    },
    {
     "data": {
      "text/plain": "        Elevation  Aspect  Slope  HDH  VDH   HDR  Hillshade_9am  \\\n0            2596      51      3  258    0   510            221   \n1            2590      56      2  212   -6   390            220   \n2            2804     139      9  268   65  3180            234   \n3            2785     155     18  242  118  3090            238   \n4            2595      45      2  153   -1   391            220   \n...           ...     ...    ...  ...  ...   ...            ...   \n581007       2396     153     20   85   17   108            240   \n581008       2391     152     19   67   12    95            240   \n581009       2386     159     17   60    7    90            236   \n581010       2384     170     15   60    5    90            230   \n581011       2383     165     13   60    4    67            231   \n\n        Hillshade_Noon  Hillshade_3pm  HDFP  ...  Soil_Type USFS7756  \\\n0                  232            148  6279  ...                   0   \n1                  235            151  6225  ...                   0   \n2                  238            135  6121  ...                   0   \n3                  238            122  6211  ...                   0   \n4                  234            150  6172  ...                   0   \n...                ...            ...   ...  ...                 ...   \n581007             237            118   837  ...                   0   \n581008             237            119   845  ...                   0   \n581009             241            130   854  ...                   0   \n581010             245            143   864  ...                   0   \n581011             244            141   875  ...                   0   \n\n        Soil_Type USFS7757  Soil_Type USFS7790  Soil_Type USFS8703  \\\n0                        0                   0                   0   \n1                        0                   0                   0   \n2                        0                   0                   0   \n3                        0                   0                   0   \n4                        0                   0                   0   \n...                    ...                 ...                 ...   \n581007                   0                   0                   0   \n581008                   0                   0                   0   \n581009                   0                   0                   0   \n581010                   0                   0                   0   \n581011                   0                   0                   0   \n\n        Soil_Type USFS8707  Soil_Type USFS8708  Soil_Type USFS8771  \\\n0                        0                   0                   0   \n1                        0                   0                   0   \n2                        0                   0                   0   \n3                        0                   0                   0   \n4                        0                   0                   0   \n...                    ...                 ...                 ...   \n581007                   0                   0                   0   \n581008                   0                   0                   0   \n581009                   0                   0                   0   \n581010                   0                   0                   0   \n581011                   0                   0                   0   \n\n        Soil_Type USFS8772  Soil_Type USFS8776  Cover_Type  \n0                        0                   0           5  \n1                        0                   0           5  \n2                        0                   0           2  \n3                        0                   0           2  \n4                        0                   0           5  \n...                    ...                 ...         ...  \n581007                   0                   0           3  \n581008                   0                   0           3  \n581009                   0                   0           3  \n581010                   0                   0           3  \n581011                   0                   0           3  \n\n[581012 rows x 55 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Elevation</th>\n      <th>Aspect</th>\n      <th>Slope</th>\n      <th>HDH</th>\n      <th>VDH</th>\n      <th>HDR</th>\n      <th>Hillshade_9am</th>\n      <th>Hillshade_Noon</th>\n      <th>Hillshade_3pm</th>\n      <th>HDFP</th>\n      <th>...</th>\n      <th>Soil_Type USFS7756</th>\n      <th>Soil_Type USFS7757</th>\n      <th>Soil_Type USFS7790</th>\n      <th>Soil_Type USFS8703</th>\n      <th>Soil_Type USFS8707</th>\n      <th>Soil_Type USFS8708</th>\n      <th>Soil_Type USFS8771</th>\n      <th>Soil_Type USFS8772</th>\n      <th>Soil_Type USFS8776</th>\n      <th>Cover_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2596</td>\n      <td>51</td>\n      <td>3</td>\n      <td>258</td>\n      <td>0</td>\n      <td>510</td>\n      <td>221</td>\n      <td>232</td>\n      <td>148</td>\n      <td>6279</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2590</td>\n      <td>56</td>\n      <td>2</td>\n      <td>212</td>\n      <td>-6</td>\n      <td>390</td>\n      <td>220</td>\n      <td>235</td>\n      <td>151</td>\n      <td>6225</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2804</td>\n      <td>139</td>\n      <td>9</td>\n      <td>268</td>\n      <td>65</td>\n      <td>3180</td>\n      <td>234</td>\n      <td>238</td>\n      <td>135</td>\n      <td>6121</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2785</td>\n      <td>155</td>\n      <td>18</td>\n      <td>242</td>\n      <td>118</td>\n      <td>3090</td>\n      <td>238</td>\n      <td>238</td>\n      <td>122</td>\n      <td>6211</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2595</td>\n      <td>45</td>\n      <td>2</td>\n      <td>153</td>\n      <td>-1</td>\n      <td>391</td>\n      <td>220</td>\n      <td>234</td>\n      <td>150</td>\n      <td>6172</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>581007</th>\n      <td>2396</td>\n      <td>153</td>\n      <td>20</td>\n      <td>85</td>\n      <td>17</td>\n      <td>108</td>\n      <td>240</td>\n      <td>237</td>\n      <td>118</td>\n      <td>837</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>581008</th>\n      <td>2391</td>\n      <td>152</td>\n      <td>19</td>\n      <td>67</td>\n      <td>12</td>\n      <td>95</td>\n      <td>240</td>\n      <td>237</td>\n      <td>119</td>\n      <td>845</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>581009</th>\n      <td>2386</td>\n      <td>159</td>\n      <td>17</td>\n      <td>60</td>\n      <td>7</td>\n      <td>90</td>\n      <td>236</td>\n      <td>241</td>\n      <td>130</td>\n      <td>854</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>581010</th>\n      <td>2384</td>\n      <td>170</td>\n      <td>15</td>\n      <td>60</td>\n      <td>5</td>\n      <td>90</td>\n      <td>230</td>\n      <td>245</td>\n      <td>143</td>\n      <td>864</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>581011</th>\n      <td>2383</td>\n      <td>165</td>\n      <td>13</td>\n      <td>60</td>\n      <td>4</td>\n      <td>67</td>\n      <td>231</td>\n      <td>244</td>\n      <td>141</td>\n      <td>875</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>581012 rows Ã— 55 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"covtype.data\",header=None)\n",
    "data.columns = [\"Elevation\",\"Aspect\",\"Slope\",\"HDH\",\"VDH\",\"HDR\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"HDFP\",\"Wilderness_Area Rawah\",\"Wilderness_Area Neota\",\"Wilderness_Area Comanche_Peak\",\"Wilderness_Area Cache_la_Poudre\",\"Soil_Type USFS2702\",\"Soil_Type USFS2703\",\"Soil_Type USFS2704\",\"Soil_Type USFS2705\",\"Soil_Type USFS2706\",\"Soil_Type USFS2717\",\"Soil_Type USFS3501\",\"Soil_Type USFS3502\",\"Soil_Type USFS4201\",\"Soil_Type USFS4703\",\"Soil_Type USFS4704\",\"Soil_Type USFS4744\",\"Soil_Type USFS4758\",\"Soil_Type USFS5101\",\"Soil_Type USFS5151\",\"Soil_Type USFS6101\",\"Soil_Type USFS6102\",\"Soil_Type USFS6731\",\"Soil_Type USFS7101\",\"Soil_Type USFS7102\",\"Soil_Type USFS7103\",\"Soil_Type USFS7201\",\"Soil_Type USFS7202\",\"Soil_Type USFS7700\",\"Soil_Type USFS7701\",\"Soil_Type USFS7702\",\"Soil_Type USFS7709\",\"Soil_Type USFS7710\",\"Soil_Type USFS7745\",\"Soil_Type USFS7746\",\"Soil_Type USFS7755\",\"Soil_Type USFS7756\",\"Soil_Type USFS7757\",\"Soil_Type USFS7790\",\"Soil_Type USFS8703\",\"Soil_Type USFS8707\",\"Soil_Type USFS8708\",\"Soil_Type USFS8771\",\"Soil_Type USFS8772\",\"Soil_Type USFS8776\",\"Cover_Type\"]\n",
    "print(data.shape)\n",
    "display(data)\n",
    "# display(data.head(10).T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### this dataset already make 2 col one-hotted so for not repeating a single operation 44 times I did revert it\n",
    "\n",
    "##### I copied this piece of code from Stack overflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def undummify(df : pd.DataFrame, prefix_sep=\" \") -> pd.DataFrame:\n",
    "    cols2collapse = {\n",
    "        item.split(prefix_sep)[0]: (prefix_sep in item) for item in df.columns\n",
    "    }\n",
    "    series_list = []\n",
    "    for col, needs_to_collapse in cols2collapse.items():\n",
    "        if needs_to_collapse:\n",
    "            undummified = (\n",
    "                df.filter(like=col)\n",
    "                .idxmax(axis=1)\n",
    "                .apply(lambda x: x.split(prefix_sep, maxsplit=1)[1])\n",
    "                .rename(col)\n",
    "            )\n",
    "            series_list.append(undummified)\n",
    "        else:\n",
    "            series_list.append(df[col])\n",
    "    undummified_df = pd.concat(series_list, axis=1)\n",
    "    return undummified_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                        464108    471404         545113    128748  \\\nElevation                 3381      3039           3318      2823   \nAspect                       0       291            353        67   \nSlope                       18        12             21        13   \nHDH                        497       361            309       170   \nVDH                        187        87             84        54   \nHDR                       4201       618           4548      2514   \nHillshade_9am              190       187            179       233   \nHillshade_Noon             203       238            198       213   \nHillshade_3pm              147       191            153       110   \nHDFP                      3736       742           1324      2879   \nWilderness_Area  Comanche_Peak     Neota  Comanche_Peak     Rawah   \nSoil_Type             USFS8776  USFS7201       USFS8772  USFS7745   \nCover_Type                   7         2              1         2   \n\n                        373677         436787         398464    181326  \\\nElevation                 3211           2797           3385      3065   \nAspect                      16            129            282         0   \nSlope                       19              9              8         4   \nHDH                        350              0            899       242   \nVDH                         36              0            179        10   \nHDR                       3254           2400           1092      3298   \nHillshade_9am              200            235            198       214   \nHillshade_Noon             196            235            241       232   \nHillshade_3pm              128            130            182       156   \nHDFP                      3129           1611           2130       485   \nWilderness_Area  Comanche_Peak  Comanche_Peak  Comanche_Peak     Rawah   \nSoil_Type             USFS7700       USFS4758       USFS7756  USFS7201   \nCover_Type                   2              2              1         1   \n\n                   195134    73676     150530         474652         369237  \\\nElevation            3209      3004      2898           2925           3153   \nAspect                157       151       124              5             30   \nSlope                   7        18         5             28             12   \nHDH                   309        90       335            150            216   \nVDH                    46         3        65             58              9   \nHDR                  5057      3458      2989            272           3072   \nHillshade_9am         228       240       229            172            216   \nHillshade_Noon        241       237       236            173            214   \nHillshade_3pm         146       120       140            127            132   \nHDFP                 1575      5949      2230            162           3150   \nWilderness_Area     Rawah     Rawah     Rawah  Comanche_Peak  Comanche_Peak   \nSoil_Type        USFS7745  USFS7745  USFS4744       USFS7757       USFS7700   \nCover_Type              1         2         2              2              2   \n\n                   386489    140891  \nElevation            3264      2985  \nAspect                113       320  \nSlope                   7        16  \nHDH                   210       120  \nVDH                    31         4  \nHDR                  1950      3837  \nHillshade_9am         233       177  \nHillshade_Noon        233       223  \nHillshade_3pm         132       185  \nHDFP                 1399      1521  \nWilderness_Area     Rawah     Rawah  \nSoil_Type        USFS7745  USFS7745  \nCover_Type              2         2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>464108</th>\n      <th>471404</th>\n      <th>545113</th>\n      <th>128748</th>\n      <th>373677</th>\n      <th>436787</th>\n      <th>398464</th>\n      <th>181326</th>\n      <th>195134</th>\n      <th>73676</th>\n      <th>150530</th>\n      <th>474652</th>\n      <th>369237</th>\n      <th>386489</th>\n      <th>140891</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Elevation</th>\n      <td>3381</td>\n      <td>3039</td>\n      <td>3318</td>\n      <td>2823</td>\n      <td>3211</td>\n      <td>2797</td>\n      <td>3385</td>\n      <td>3065</td>\n      <td>3209</td>\n      <td>3004</td>\n      <td>2898</td>\n      <td>2925</td>\n      <td>3153</td>\n      <td>3264</td>\n      <td>2985</td>\n    </tr>\n    <tr>\n      <th>Aspect</th>\n      <td>0</td>\n      <td>291</td>\n      <td>353</td>\n      <td>67</td>\n      <td>16</td>\n      <td>129</td>\n      <td>282</td>\n      <td>0</td>\n      <td>157</td>\n      <td>151</td>\n      <td>124</td>\n      <td>5</td>\n      <td>30</td>\n      <td>113</td>\n      <td>320</td>\n    </tr>\n    <tr>\n      <th>Slope</th>\n      <td>18</td>\n      <td>12</td>\n      <td>21</td>\n      <td>13</td>\n      <td>19</td>\n      <td>9</td>\n      <td>8</td>\n      <td>4</td>\n      <td>7</td>\n      <td>18</td>\n      <td>5</td>\n      <td>28</td>\n      <td>12</td>\n      <td>7</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>HDH</th>\n      <td>497</td>\n      <td>361</td>\n      <td>309</td>\n      <td>170</td>\n      <td>350</td>\n      <td>0</td>\n      <td>899</td>\n      <td>242</td>\n      <td>309</td>\n      <td>90</td>\n      <td>335</td>\n      <td>150</td>\n      <td>216</td>\n      <td>210</td>\n      <td>120</td>\n    </tr>\n    <tr>\n      <th>VDH</th>\n      <td>187</td>\n      <td>87</td>\n      <td>84</td>\n      <td>54</td>\n      <td>36</td>\n      <td>0</td>\n      <td>179</td>\n      <td>10</td>\n      <td>46</td>\n      <td>3</td>\n      <td>65</td>\n      <td>58</td>\n      <td>9</td>\n      <td>31</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>HDR</th>\n      <td>4201</td>\n      <td>618</td>\n      <td>4548</td>\n      <td>2514</td>\n      <td>3254</td>\n      <td>2400</td>\n      <td>1092</td>\n      <td>3298</td>\n      <td>5057</td>\n      <td>3458</td>\n      <td>2989</td>\n      <td>272</td>\n      <td>3072</td>\n      <td>1950</td>\n      <td>3837</td>\n    </tr>\n    <tr>\n      <th>Hillshade_9am</th>\n      <td>190</td>\n      <td>187</td>\n      <td>179</td>\n      <td>233</td>\n      <td>200</td>\n      <td>235</td>\n      <td>198</td>\n      <td>214</td>\n      <td>228</td>\n      <td>240</td>\n      <td>229</td>\n      <td>172</td>\n      <td>216</td>\n      <td>233</td>\n      <td>177</td>\n    </tr>\n    <tr>\n      <th>Hillshade_Noon</th>\n      <td>203</td>\n      <td>238</td>\n      <td>198</td>\n      <td>213</td>\n      <td>196</td>\n      <td>235</td>\n      <td>241</td>\n      <td>232</td>\n      <td>241</td>\n      <td>237</td>\n      <td>236</td>\n      <td>173</td>\n      <td>214</td>\n      <td>233</td>\n      <td>223</td>\n    </tr>\n    <tr>\n      <th>Hillshade_3pm</th>\n      <td>147</td>\n      <td>191</td>\n      <td>153</td>\n      <td>110</td>\n      <td>128</td>\n      <td>130</td>\n      <td>182</td>\n      <td>156</td>\n      <td>146</td>\n      <td>120</td>\n      <td>140</td>\n      <td>127</td>\n      <td>132</td>\n      <td>132</td>\n      <td>185</td>\n    </tr>\n    <tr>\n      <th>HDFP</th>\n      <td>3736</td>\n      <td>742</td>\n      <td>1324</td>\n      <td>2879</td>\n      <td>3129</td>\n      <td>1611</td>\n      <td>2130</td>\n      <td>485</td>\n      <td>1575</td>\n      <td>5949</td>\n      <td>2230</td>\n      <td>162</td>\n      <td>3150</td>\n      <td>1399</td>\n      <td>1521</td>\n    </tr>\n    <tr>\n      <th>Wilderness_Area</th>\n      <td>Comanche_Peak</td>\n      <td>Neota</td>\n      <td>Comanche_Peak</td>\n      <td>Rawah</td>\n      <td>Comanche_Peak</td>\n      <td>Comanche_Peak</td>\n      <td>Comanche_Peak</td>\n      <td>Rawah</td>\n      <td>Rawah</td>\n      <td>Rawah</td>\n      <td>Rawah</td>\n      <td>Comanche_Peak</td>\n      <td>Comanche_Peak</td>\n      <td>Rawah</td>\n      <td>Rawah</td>\n    </tr>\n    <tr>\n      <th>Soil_Type</th>\n      <td>USFS8776</td>\n      <td>USFS7201</td>\n      <td>USFS8772</td>\n      <td>USFS7745</td>\n      <td>USFS7700</td>\n      <td>USFS4758</td>\n      <td>USFS7756</td>\n      <td>USFS7201</td>\n      <td>USFS7745</td>\n      <td>USFS7745</td>\n      <td>USFS4744</td>\n      <td>USFS7757</td>\n      <td>USFS7700</td>\n      <td>USFS7745</td>\n      <td>USFS7745</td>\n    </tr>\n    <tr>\n      <th>Cover_Type</th>\n      <td>7</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(undummify(data).sample(15,random_state=10).T)\n",
    "data = undummify(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116202, 13) (81341, 13) (34861, 13) (464810, 13)\n"
     ]
    }
   ],
   "source": [
    "df = data.sample(frac=0.2, random_state=1337)\n",
    "train_df = df.sample(frac=0.7, random_state=1337)\n",
    "val_df = df.drop(train_df.index)\n",
    "test_df = data.drop(df.index)\n",
    "print(df.shape, train_df.shape ,val_df.shape ,test_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### turning Pandas_DataFrame to TF_DataSet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(dataframe:pd.DataFrame) -> tf.data.Dataset:\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"Cover_Type\")\n",
    "    labels = keras.utils.to_categorical(labels)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "\n",
    "    return ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_ds = dataframe_to_dataset(train_df)\n",
    "val_ds = dataframe_to_dataset(val_df)\n",
    "test_ds = dataframe_to_dataset(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'Elevation': <tf.Tensor: shape=(), dtype=int64, numpy=3002>, 'Aspect': <tf.Tensor: shape=(), dtype=int64, numpy=356>, 'Slope': <tf.Tensor: shape=(), dtype=int64, numpy=19>, 'HDH': <tf.Tensor: shape=(), dtype=int64, numpy=127>, 'VDH': <tf.Tensor: shape=(), dtype=int64, numpy=15>, 'HDR': <tf.Tensor: shape=(), dtype=int64, numpy=430>, 'Hillshade_9am': <tf.Tensor: shape=(), dtype=int64, numpy=186>, 'Hillshade_Noon': <tf.Tensor: shape=(), dtype=int64, numpy=203>, 'Hillshade_3pm': <tf.Tensor: shape=(), dtype=int64, numpy=152>, 'HDFP': <tf.Tensor: shape=(), dtype=int64, numpy=1222>, 'Wilderness_Area': <tf.Tensor: shape=(), dtype=string, numpy=b'Comanche_Peak'>, 'Soil_Type': <tf.Tensor: shape=(), dtype=string, numpy=b'USFS7756'>}\n",
      "Target: tf.Tensor([0. 0. 1. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)\n",
    "test_ds = val_ds.batch(32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuring a FeatureSpace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from keras.utils import FeatureSpace\n",
    "\n",
    "feature_space = FeatureSpace(\n",
    "    features={\n",
    "        # Categorical features encoded as integers\n",
    "        # \"sex\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        # Categorical feature encoded as string\n",
    "        \"Wilderness_Area\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"Soil_Type\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        # Numerical features to normalize\n",
    "        # \"age\": FeatureSpace.float_discretized(num_bins=30),\n",
    "        # Numerical features to normalize\n",
    "        \"Elevation\": FeatureSpace.float_normalized(),\n",
    "        \"Aspect\": FeatureSpace.float_normalized(),\n",
    "        \"Slope\": FeatureSpace.float_normalized(),\n",
    "        \"HDH\": FeatureSpace.float_normalized(),\n",
    "        \"VDH\": FeatureSpace.float_normalized(),\n",
    "        \"HDR\": FeatureSpace.float_normalized(),\n",
    "        \"Hillshade_9am\": FeatureSpace.float_normalized(),\n",
    "        \"Hillshade_Noon\": FeatureSpace.float_normalized(),\n",
    "        \"Hillshade_3pm\": FeatureSpace.float_normalized(),\n",
    "        \"HDFP\": FeatureSpace.float_normalized(),\n",
    "\n",
    "    },\n",
    "    # Specify feature cross with a custom crossing dim.\n",
    "    # crosses=[\n",
    "    #     FeatureSpace.cross(feature_names=(\"Soil_Type\", \"Wilderness_Area\"), crossing_dim=32),\n",
    "    # ],\n",
    "    output_mode=\"concat\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adapt that Config to train_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_ds_with_no_labels = train_ds.map(lambda x, _: x)\n",
    "feature_space.adapt(train_ds_with_no_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_x.shape: (32, 53)\n",
      "preprocessed_x.dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "for x, _ in train_ds.take(1):\n",
    "    preprocessed_x = feature_space(x)\n",
    "    print(\"preprocessed_x.shape:\", preprocessed_x.shape)\n",
    "    print(\"preprocessed_x.dtype:\", preprocessed_x.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Assigning Var to each transformed DSs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "preprocessed_train_ds = train_ds.map(\n",
    "    lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "preprocessed_train_ds = preprocessed_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "preprocessed_val_ds = val_ds.map(\n",
    "    lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "preprocessed_val_ds = preprocessed_val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "preprocessed_test_ds = test_ds.map(\n",
    "    lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "preprocessed_test_ds = preprocessed_test_ds.prefetch(tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ANNs Modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### My first and reference model which I made luckily and used model from Keras doc, and add 3 more layers to it which got 83% accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.7023 - accuracy: 0.7053 - val_loss: 0.6277 - val_accuracy: 0.7236\n",
      "Epoch 2/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5989 - accuracy: 0.7445 - val_loss: 0.5788 - val_accuracy: 0.7509\n",
      "Epoch 3/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5648 - accuracy: 0.7581 - val_loss: 0.5848 - val_accuracy: 0.7467\n",
      "Epoch 4/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5426 - accuracy: 0.7653 - val_loss: 0.5381 - val_accuracy: 0.7700\n",
      "Epoch 5/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5238 - accuracy: 0.7756 - val_loss: 0.5385 - val_accuracy: 0.7683\n",
      "Epoch 6/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5086 - accuracy: 0.7820 - val_loss: 0.5138 - val_accuracy: 0.7801\n",
      "Epoch 7/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4948 - accuracy: 0.7894 - val_loss: 0.5149 - val_accuracy: 0.7767\n",
      "Epoch 8/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4841 - accuracy: 0.7946 - val_loss: 0.4882 - val_accuracy: 0.7915\n",
      "Epoch 9/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4736 - accuracy: 0.7985 - val_loss: 0.4834 - val_accuracy: 0.7945\n",
      "Epoch 10/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4638 - accuracy: 0.8034 - val_loss: 0.4888 - val_accuracy: 0.7938\n",
      "Epoch 11/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4546 - accuracy: 0.8071 - val_loss: 0.4588 - val_accuracy: 0.8053\n",
      "Epoch 12/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4486 - accuracy: 0.8107 - val_loss: 0.4546 - val_accuracy: 0.8097\n",
      "Epoch 13/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4414 - accuracy: 0.8130 - val_loss: 0.4729 - val_accuracy: 0.7982\n",
      "Epoch 14/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4363 - accuracy: 0.8146 - val_loss: 0.4504 - val_accuracy: 0.8089\n",
      "Epoch 15/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4322 - accuracy: 0.8185 - val_loss: 0.4405 - val_accuracy: 0.8129\n",
      "Epoch 16/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4248 - accuracy: 0.8212 - val_loss: 0.4378 - val_accuracy: 0.8162\n",
      "Epoch 17/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4204 - accuracy: 0.8217 - val_loss: 0.4412 - val_accuracy: 0.8147\n",
      "Epoch 18/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4159 - accuracy: 0.8245 - val_loss: 0.4240 - val_accuracy: 0.8209\n",
      "Epoch 19/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4113 - accuracy: 0.8264 - val_loss: 0.4263 - val_accuracy: 0.8222\n",
      "Epoch 20/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4057 - accuracy: 0.8299 - val_loss: 0.4301 - val_accuracy: 0.8216\n",
      "Epoch 21/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4005 - accuracy: 0.8311 - val_loss: 0.4171 - val_accuracy: 0.8257\n",
      "Epoch 22/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3963 - accuracy: 0.8333 - val_loss: 0.4179 - val_accuracy: 0.8263\n",
      "Epoch 23/25\n",
      "2542/2542 [==============================] - 3s 992us/step - loss: 0.3928 - accuracy: 0.8334 - val_loss: 0.4144 - val_accuracy: 0.8271\n",
      "Epoch 24/25\n",
      "2542/2542 [==============================] - 3s 971us/step - loss: 0.3889 - accuracy: 0.8354 - val_loss: 0.4109 - val_accuracy: 0.8260\n",
      "Epoch 25/25\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3852 - accuracy: 0.8387 - val_loss: 0.4108 - val_accuracy: 0.8282\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2a49781c520>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_inputs = feature_space.get_inputs()\n",
    "encoded_features = feature_space.get_encoded_features()\n",
    "\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(encoded_features)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "predictions = keras.layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "training_model = keras.Model(inputs=encoded_features, outputs=predictions)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "inference_model = keras.Model(inputs=dict_inputs, outputs=predictions)\n",
    "\n",
    "\n",
    "\n",
    "training_model.fit(\n",
    "    preprocessed_train_ds, epochs=25, validation_data=preprocessed_val_ds, verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In second one, I used dropout and deleted 64-nodes layer to accel its computing without adding more complex layers but accuracy decreased too much"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.8148 - accuracy: 0.6693 - val_loss: 0.6581 - val_accuracy: 0.7204\n",
      "Epoch 2/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.6709 - accuracy: 0.7228 - val_loss: 0.6833 - val_accuracy: 0.6899\n",
      "Epoch 3/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.6452 - accuracy: 0.7326 - val_loss: 0.6514 - val_accuracy: 0.7041\n",
      "Epoch 4/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.6271 - accuracy: 0.7407 - val_loss: 0.6308 - val_accuracy: 0.7287\n",
      "Epoch 5/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.6176 - accuracy: 0.7432 - val_loss: 0.6513 - val_accuracy: 0.7187\n",
      "Epoch 6/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.6086 - accuracy: 0.7468 - val_loss: 0.6656 - val_accuracy: 0.7124\n",
      "Epoch 7/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.6020 - accuracy: 0.7515 - val_loss: 0.6342 - val_accuracy: 0.7266\n",
      "Epoch 8/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5982 - accuracy: 0.7493 - val_loss: 0.6455 - val_accuracy: 0.7238\n",
      "Epoch 9/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5937 - accuracy: 0.7512 - val_loss: 0.6550 - val_accuracy: 0.7094\n",
      "Epoch 10/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5887 - accuracy: 0.7541 - val_loss: 0.6127 - val_accuracy: 0.7404\n",
      "Epoch 11/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5867 - accuracy: 0.7546 - val_loss: 0.6863 - val_accuracy: 0.6904\n",
      "Epoch 12/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5828 - accuracy: 0.7556 - val_loss: 0.6399 - val_accuracy: 0.7241\n",
      "Epoch 13/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5801 - accuracy: 0.7567 - val_loss: 0.6295 - val_accuracy: 0.7210\n",
      "Epoch 14/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5783 - accuracy: 0.7566 - val_loss: 0.6144 - val_accuracy: 0.7339\n",
      "Epoch 15/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5766 - accuracy: 0.7580 - val_loss: 0.6489 - val_accuracy: 0.7276\n",
      "Epoch 16/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5733 - accuracy: 0.7603 - val_loss: 0.6678 - val_accuracy: 0.7057\n",
      "Epoch 17/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5734 - accuracy: 0.7598 - val_loss: 0.6742 - val_accuracy: 0.7016\n",
      "Epoch 18/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5698 - accuracy: 0.7612 - val_loss: 0.6388 - val_accuracy: 0.7248\n",
      "Epoch 19/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5709 - accuracy: 0.7612 - val_loss: 0.6583 - val_accuracy: 0.6988\n",
      "Epoch 20/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5681 - accuracy: 0.7609 - val_loss: 0.6613 - val_accuracy: 0.6930\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2a497839160>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_inputs = feature_space.get_inputs()\n",
    "encoded_features = feature_space.get_encoded_features()\n",
    "\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(encoded_features)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "predictions = keras.layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "training_model = keras.Model(inputs=encoded_features, outputs=predictions)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "inference_model = keras.Model(inputs=dict_inputs, outputs=predictions)\n",
    "\n",
    "\n",
    "\n",
    "training_model.fit(\n",
    "    preprocessed_train_ds, epochs=20, validation_data=preprocessed_val_ds, verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In 3rd one, I added Dropout layer but didn't delete any layer also got pretty good accuracy (82%)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.7389 - accuracy: 0.6960 - val_loss: 0.6222 - val_accuracy: 0.7356\n",
      "Epoch 2/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.6236 - accuracy: 0.7359 - val_loss: 0.5966 - val_accuracy: 0.7447\n",
      "Epoch 3/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5957 - accuracy: 0.7484 - val_loss: 0.5680 - val_accuracy: 0.7557\n",
      "Epoch 4/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5749 - accuracy: 0.7570 - val_loss: 0.5506 - val_accuracy: 0.7650\n",
      "Epoch 5/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5586 - accuracy: 0.7642 - val_loss: 0.5460 - val_accuracy: 0.7648\n",
      "Epoch 6/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5449 - accuracy: 0.7690 - val_loss: 0.5252 - val_accuracy: 0.7734\n",
      "Epoch 7/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5352 - accuracy: 0.7737 - val_loss: 0.5195 - val_accuracy: 0.7775\n",
      "Epoch 8/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5256 - accuracy: 0.7766 - val_loss: 0.5146 - val_accuracy: 0.7812\n",
      "Epoch 9/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5184 - accuracy: 0.7792 - val_loss: 0.5041 - val_accuracy: 0.7861\n",
      "Epoch 10/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5125 - accuracy: 0.7816 - val_loss: 0.4989 - val_accuracy: 0.7877\n",
      "Epoch 11/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5063 - accuracy: 0.7842 - val_loss: 0.4953 - val_accuracy: 0.7885\n",
      "Epoch 12/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5007 - accuracy: 0.7853 - val_loss: 0.5046 - val_accuracy: 0.7799\n",
      "Epoch 13/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4951 - accuracy: 0.7883 - val_loss: 0.4815 - val_accuracy: 0.7950\n",
      "Epoch 14/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4906 - accuracy: 0.7917 - val_loss: 0.4728 - val_accuracy: 0.7973\n",
      "Epoch 15/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4864 - accuracy: 0.7929 - val_loss: 0.4738 - val_accuracy: 0.7986\n",
      "Epoch 16/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4836 - accuracy: 0.7925 - val_loss: 0.4731 - val_accuracy: 0.7944\n",
      "Epoch 17/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4793 - accuracy: 0.7958 - val_loss: 0.4668 - val_accuracy: 0.8029\n",
      "Epoch 18/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4756 - accuracy: 0.7965 - val_loss: 0.4632 - val_accuracy: 0.8053\n",
      "Epoch 19/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4751 - accuracy: 0.7982 - val_loss: 0.4629 - val_accuracy: 0.8049\n",
      "Epoch 20/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4704 - accuracy: 0.8001 - val_loss: 0.4616 - val_accuracy: 0.8061\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2a49ab139a0>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_inputs = feature_space.get_inputs()\n",
    "encoded_features = feature_space.get_encoded_features()\n",
    "\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(encoded_features)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "predictions = keras.layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "training_model = keras.Model(inputs=encoded_features, outputs=predictions)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "inference_model = keras.Model(inputs=dict_inputs, outputs=predictions)\n",
    "\n",
    "\n",
    "\n",
    "training_model.fit(\n",
    "    preprocessed_train_ds, epochs=20, validation_data=preprocessed_val_ds, verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In 4th one, I added a 64-nodes layer and also included a Dropout layer, but it just got worse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2542/2542 [==============================] - 5s 1ms/step - loss: 0.7542 - accuracy: 0.6856 - val_loss: 0.6396 - val_accuracy: 0.7251\n",
      "Epoch 2/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.6334 - accuracy: 0.7363 - val_loss: 0.6083 - val_accuracy: 0.7477\n",
      "Epoch 3/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.6000 - accuracy: 0.7499 - val_loss: 0.5907 - val_accuracy: 0.7544\n",
      "Epoch 4/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5794 - accuracy: 0.7577 - val_loss: 0.5507 - val_accuracy: 0.7649\n",
      "Epoch 5/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5615 - accuracy: 0.7674 - val_loss: 0.5433 - val_accuracy: 0.7700\n",
      "Epoch 6/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5501 - accuracy: 0.7720 - val_loss: 0.5272 - val_accuracy: 0.7787\n",
      "Epoch 7/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5393 - accuracy: 0.7748 - val_loss: 0.5193 - val_accuracy: 0.7775\n",
      "Epoch 8/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5331 - accuracy: 0.7768 - val_loss: 0.5171 - val_accuracy: 0.7824\n",
      "Epoch 9/20\n",
      "2542/2542 [==============================] - 5s 2ms/step - loss: 0.5254 - accuracy: 0.7824 - val_loss: 0.5007 - val_accuracy: 0.7861\n",
      "Epoch 10/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5186 - accuracy: 0.7839 - val_loss: 0.5033 - val_accuracy: 0.7908\n",
      "Epoch 11/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5147 - accuracy: 0.7865 - val_loss: 0.4969 - val_accuracy: 0.7915\n",
      "Epoch 12/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5092 - accuracy: 0.7876 - val_loss: 0.4871 - val_accuracy: 0.7988\n",
      "Epoch 13/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5062 - accuracy: 0.7891 - val_loss: 0.4891 - val_accuracy: 0.7928\n",
      "Epoch 14/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5016 - accuracy: 0.7916 - val_loss: 0.4814 - val_accuracy: 0.7995\n",
      "Epoch 15/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.4973 - accuracy: 0.7924 - val_loss: 0.4760 - val_accuracy: 0.8025\n",
      "Epoch 16/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4962 - accuracy: 0.7922 - val_loss: 0.4719 - val_accuracy: 0.8028\n",
      "Epoch 17/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.4903 - accuracy: 0.7958 - val_loss: 0.4694 - val_accuracy: 0.8039\n",
      "Epoch 18/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4888 - accuracy: 0.7964 - val_loss: 0.4671 - val_accuracy: 0.8052\n",
      "Epoch 19/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.4830 - accuracy: 0.7989 - val_loss: 0.4763 - val_accuracy: 0.7987\n",
      "Epoch 20/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.4818 - accuracy: 0.7983 - val_loss: 0.4637 - val_accuracy: 0.8063\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2a49bde0e50>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_inputs = feature_space.get_inputs()\n",
    "encoded_features = feature_space.get_encoded_features()\n",
    "\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(encoded_features)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "predictions = keras.layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "training_model = keras.Model(inputs=encoded_features, outputs=predictions)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "inference_model = keras.Model(inputs=dict_inputs, outputs=predictions)\n",
    "\n",
    "\n",
    "\n",
    "training_model.fit(\n",
    "    preprocessed_train_ds, epochs=20, validation_data=preprocessed_val_ds, verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In 5th one, I added 2 more layers to complex model without any dropout layers, and got amazingly go 85% accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.6802 - accuracy: 0.7156 - val_loss: 0.6193 - val_accuracy: 0.7328\n",
      "Epoch 2/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5872 - accuracy: 0.7505 - val_loss: 0.5744 - val_accuracy: 0.7535\n",
      "Epoch 3/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5492 - accuracy: 0.7664 - val_loss: 0.5479 - val_accuracy: 0.7635\n",
      "Epoch 4/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5210 - accuracy: 0.7776 - val_loss: 0.5146 - val_accuracy: 0.7768\n",
      "Epoch 5/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4977 - accuracy: 0.7884 - val_loss: 0.4950 - val_accuracy: 0.7874\n",
      "Epoch 6/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4760 - accuracy: 0.7971 - val_loss: 0.4773 - val_accuracy: 0.7958\n",
      "Epoch 7/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4597 - accuracy: 0.8065 - val_loss: 0.4632 - val_accuracy: 0.8049\n",
      "Epoch 8/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4444 - accuracy: 0.8114 - val_loss: 0.4518 - val_accuracy: 0.8059\n",
      "Epoch 9/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4275 - accuracy: 0.8208 - val_loss: 0.4374 - val_accuracy: 0.8160\n",
      "Epoch 10/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4135 - accuracy: 0.8270 - val_loss: 0.4313 - val_accuracy: 0.8208\n",
      "Epoch 11/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4020 - accuracy: 0.8318 - val_loss: 0.4279 - val_accuracy: 0.8234\n",
      "Epoch 12/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3908 - accuracy: 0.8382 - val_loss: 0.4098 - val_accuracy: 0.8285\n",
      "Epoch 13/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3799 - accuracy: 0.8422 - val_loss: 0.4028 - val_accuracy: 0.8331\n",
      "Epoch 14/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3728 - accuracy: 0.8455 - val_loss: 0.4149 - val_accuracy: 0.8271\n",
      "Epoch 15/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3651 - accuracy: 0.8490 - val_loss: 0.3973 - val_accuracy: 0.8360\n",
      "Epoch 16/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3558 - accuracy: 0.8532 - val_loss: 0.3872 - val_accuracy: 0.8405\n",
      "Epoch 17/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3495 - accuracy: 0.8558 - val_loss: 0.3770 - val_accuracy: 0.8467\n",
      "Epoch 18/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3408 - accuracy: 0.8603 - val_loss: 0.3756 - val_accuracy: 0.8473\n",
      "Epoch 19/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3335 - accuracy: 0.8633 - val_loss: 0.3698 - val_accuracy: 0.8494\n",
      "Epoch 20/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3269 - accuracy: 0.8665 - val_loss: 0.3721 - val_accuracy: 0.8514\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2a49e028400>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_inputs = feature_space.get_inputs()\n",
    "encoded_features = feature_space.get_encoded_features()\n",
    "\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(encoded_features)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "predictions = keras.layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "training_model = keras.Model(inputs=encoded_features, outputs=predictions)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "inference_model = keras.Model(inputs=dict_inputs, outputs=predictions)\n",
    "\n",
    "\n",
    "\n",
    "training_model.fit(\n",
    "    preprocessed_train_ds, epochs=20, validation_data=preprocessed_val_ds, verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### And my last model, which I just wanted to test if i keep complexity in a same level as reference just adding Dropout to descending part of model what would happen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.7012 - accuracy: 0.7071 - val_loss: 0.6153 - val_accuracy: 0.7350\n",
      "Epoch 2/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5967 - accuracy: 0.7463 - val_loss: 0.5765 - val_accuracy: 0.7522\n",
      "Epoch 3/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5633 - accuracy: 0.7612 - val_loss: 0.5471 - val_accuracy: 0.7680\n",
      "Epoch 4/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.5372 - accuracy: 0.7745 - val_loss: 0.5315 - val_accuracy: 0.7778\n",
      "Epoch 5/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5170 - accuracy: 0.7812 - val_loss: 0.5068 - val_accuracy: 0.7865\n",
      "Epoch 6/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.5020 - accuracy: 0.7874 - val_loss: 0.4855 - val_accuracy: 0.7945\n",
      "Epoch 7/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4882 - accuracy: 0.7944 - val_loss: 0.4845 - val_accuracy: 0.7956\n",
      "Epoch 8/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4776 - accuracy: 0.7984 - val_loss: 0.4696 - val_accuracy: 0.8034\n",
      "Epoch 9/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4673 - accuracy: 0.8037 - val_loss: 0.4582 - val_accuracy: 0.8104\n",
      "Epoch 10/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4593 - accuracy: 0.8071 - val_loss: 0.4501 - val_accuracy: 0.8141\n",
      "Epoch 11/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.4532 - accuracy: 0.8090 - val_loss: 0.4445 - val_accuracy: 0.8142\n",
      "Epoch 12/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4461 - accuracy: 0.8123 - val_loss: 0.4407 - val_accuracy: 0.8156\n",
      "Epoch 13/20\n",
      "2542/2542 [==============================] - 4s 1ms/step - loss: 0.4414 - accuracy: 0.8158 - val_loss: 0.4277 - val_accuracy: 0.8260\n",
      "Epoch 14/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4357 - accuracy: 0.8175 - val_loss: 0.4261 - val_accuracy: 0.8217\n",
      "Epoch 15/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4306 - accuracy: 0.8193 - val_loss: 0.4224 - val_accuracy: 0.8271\n",
      "Epoch 16/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4257 - accuracy: 0.8225 - val_loss: 0.4312 - val_accuracy: 0.8197\n",
      "Epoch 17/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4240 - accuracy: 0.8225 - val_loss: 0.4235 - val_accuracy: 0.8239\n",
      "Epoch 18/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4211 - accuracy: 0.8241 - val_loss: 0.4180 - val_accuracy: 0.8246\n",
      "Epoch 19/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4157 - accuracy: 0.8263 - val_loss: 0.4171 - val_accuracy: 0.8258\n",
      "Epoch 20/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.4144 - accuracy: 0.8264 - val_loss: 0.4136 - val_accuracy: 0.8287\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2a4a0258460>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_inputs = feature_space.get_inputs()\n",
    "encoded_features = feature_space.get_encoded_features()\n",
    "\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(encoded_features)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.concatenate([x,encoded_features])\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(0.2)(x)\n",
    "predictions = keras.layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "training_model = keras.Model(inputs=encoded_features, outputs=predictions)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "inference_model = keras.Model(inputs=dict_inputs, outputs=predictions)\n",
    "\n",
    "\n",
    "\n",
    "training_model.fit(\n",
    "    preprocessed_train_ds, epochs=20, validation_data=preprocessed_val_ds, verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}